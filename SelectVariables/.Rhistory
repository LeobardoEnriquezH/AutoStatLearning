bd_int_less <- bd[, -1]
metodo_lasso <- glmnet(X,Y, family = gaussian(link = "identity"), nlambda = 100)
Lista_modelos <- list()
Lista_BIC <- list()
final <- length(metodo_lasso$lambda)
print(final)
for (i in 1:final) {
coeficientes <- coef(metodo_lasso)[, i] != 0
matriz_variables_X <- X[, coeficientes[-1]] # Excluir el intercepto
# Ajustar el modelo con las variables seleccionadas por la penalizacion Lasso
ajuste_lasso <- glm(formula = Y ~ ., family = gaussian(link = "identity"), data = data.frame(matriz_variables_X, Y))
Lista_modelos[[i]] <- ajuste_lasso
Lista_BIC[[i]] <- BIC(ajuste_lasso)
}
# Se busca el indice del modelo con el minimo BIC, usamos la funcion unlist para deshacer la lista
min_bic_indice <- which.min(unlist(Lista_BIC))
# Se obtiene el modelo optimo y sus coeficientes
modelo_seleccionado <- Lista_modelos[[min_bic_indice]]
coeficientes <- coefficients(modelo_seleccionado)
# Se imprimen los coeficientes del modelo optimo y su BIC
print(coeficientes)
Ajuste_ModeloLasso <- lm(formula = brozek ~ age + height + abdom + wrist, data = fat)
#print(BIC(modelo_seleccionado))
BIC_Ajuste_ModeloLasso<-BIC(Ajuste_ModeloLasso)
sprintf("BIC: %f", BIC_Ajuste_ModeloLasso)
#summary
#summary(modelo_seleccionado)
#summary(Ajuste_ModeloLasso)
forward_interacciones <- regsubsets(brozek ~ . ^2,
data = fat,
method = "forward",
nvmax = 14)
#Usando la siguiente grÃ¡fica encontramos 4 valores
#plot(forward_interacciones, scale = "bic")
#Obtenemos las variables con las que debemos trabajar
coef(forward_interacciones, 3)
### Ajuste con las covariables que entran al modelo bajo la selecciÃ³n backward
Ajuste_forward2 <- lm(formula = brozek ~ abdom + height:wrist + chest:hip,
data = fat)
#summary(Ajuste_forward2)
### ComparaciÃ³n de BIC.
BIC_Ajuste_forward2<-BIC(Ajuste_forward2)
sprintf("BIC: %f", BIC_Ajuste_forward2)
Metodo_backward_interacciones <- regsubsets(brozek ~ . ^2,
data = fat,
method = "backward",
nvmax = 14)
#Usando la siguiente grÃ¡fica encontramos 4 valores
#plot(Metodo_backward_interacciones, scale = "bic")
#Obtenemos las variables con las que debemos trabajar
coef(Metodo_backward_interacciones, 4)
### Ajuste con las covariables que entran al modelo bajo la selecciÃ³n backward
Ajuste_backward2 <- lm(formula = brozek ~ hip + height:hip + neck:abdom + neck:hip,
data = fat)
#summary(Ajuste_backward2)
### ComparaciÃ³n de BIC.
BIC_Ajuste_backward2<-BIC(Ajuste_backward2)
sprintf("BIC: %f", BIC_Ajuste_backward2)
#Matriz diseño considerando interacciones
X2 <- model.matrix(object = brozek ~ .^2, data = fat)
#Le quitamos el intercepto
X2_aux <- X2[,-1]
#Variable y
y <- fat$brozek
#Realizamos la penalizacion lasso
lasso_inter <- glmnet(X2_aux, y, family <- gaussian(link = "identity"), nlambda = 100)
#Para eso, los coeficientes los vamos a utilizar, y buscaremos los
#coeficientes que no son 0 en cada iteraciÃ³n, y lo haremos un dataframe
coeficientes2 <- data.frame(t(as.matrix(coef(lasso_inter)!=0)))
#Como podemos tener una gran cantidad de valores repetidos con esto,
#vamos a aplica la funciÃ³n unique, que nos ayudarÃ¡ a eliminar valores repetidos
coeficientes2 <- unique(coeficientes2)
#Con esto, vamos a obtener la combinaciÃ³n que tiene el menor BIC
BIC_lasso_comp<-sapply(1:length(coeficientes2$X.Intercept.), function(x){
BIC(glm(formula = y ~ X2[,unlist(coeficientes2[x,])] - 1, family = gaussian))})
#Utilizamos la segunda forma porque la primera tiene algunas complicaciones
best_lasso_comp2 <- glm(formula = y ~ X2_aux[,unlist(coeficientes2[which.min(BIC_lasso_comp),c(-1)])], family = gaussian)
#print(coef(best_lasso_comp2))
AjusteLasso_Interacciones <- glm(formula = brozek ~ abdom + age:abdom + age:thigh + height:wrist, data = fat)
AjusteLasso_Interacciones$coefficients
#Verificamos el BIC
#summary(BIC_lasso_comp)
#summary(best_lasso_comp2)
#summary(AjusteLasso_Interacciones)
#BIC(best_lasso_comp2)
BIC_AjusteLasso_Interacciones<-BIC(AjusteLasso_Interacciones)
sprintf("BIC: %f", BIC_AjusteLasso_Interacciones)
modelo_gamma_x <- glm(formula = brozek ~ height + abdom  + wrist,
family = Gamma(link = "identity"), data = fat)
modelo_gamma_log <- glm(formula = brozek ~ height + abdom  + wrist,
family = Gamma(link = "log"), data = fat)
#Metodo stepwise con backward
modelo_gamma_x2 <- glm(formula = brozek ~ weight + abdom + wrist, family = Gamma(link = "identity"), data = fat)
modelo_gamma_log2 <- glm(formula = brozek ~ weight + abdom + wrist, family = Gamma(link = "log"), data = fat)
#Forward
modelo_gamma_x3 <- glm(formula = brozek ~ abdom + height + abdom + wrist, family = Gamma(link = "identity"), data = fat)
modelo_gamma_log3 <- glm(formula = brozek ~ abdom + height + abdom + wrist, family = Gamma(link = "log"), data = fat)
#Segundo inciso tomando las mejoras
#Backward
modelo_gamma_x4 <- glm(formula = brozek ~ hip + height:hip + neck:abdom + neck:hip, family = Gamma(link = "identity"), data = fat)
modelo_gamma_log4 <- glm(formula = brozek ~ hip + height:hip + neck:abdom + neck:hip, family = Gamma(link = "log"), data = fat)
#Forward
modelo_gamma_x5 <- glm(formula = brozek ~ abdom + height:wrist + chest:hip, family = Gamma(link = "identity"), data = fat)
modelo_gamma_log5 <- glm(formula = brozek ~ abdom + height:wrist + chest:hip, family = Gamma(link = "log"), data = fat)
#Lasso
modelo_gamma_x6 <- glm(formula = brozek ~ abdom + age:abdom + age:thigh + height:wrist, family = Gamma(link = "identity"), data = fat)
modelo_gamma_log6 <- glm(formula = brozek ~ abdom + age:abdom + age:thigh  + height:wrist, family = Gamma(link = "log"), data = fat)
#BIC's
#print(BIC(modelo_gamma_x))
#print(BIC(modelo_gamma_log))
#print(BIC(modelo_gamma_x2))
#print(BIC(modelo_gamma_log2))
#print(BIC(modelo_gamma_x3))
#print(BIC(modelo_gamma_log3))
#print(BIC(modelo_gamma_x4))
#print(BIC(modelo_gamma_log4))
#print(BIC(modelo_gamma_x5))
#print(BIC(modelo_gamma_log5))
#print(BIC(modelo_gamma_x6))
#print(BIC(modelo_gamma_log6))
#Mejor modelo segun el criterio del BIC
Lista_bic_modelos <- c(BIC(modelo_gamma_x), BIC(modelo_gamma_log), BIC(modelo_gamma_x2), BIC(modelo_gamma_log2), BIC(modelo_gamma_x3), BIC(modelo_gamma_log3), BIC(modelo_gamma_x4), BIC(modelo_gamma_log4), BIC(modelo_gamma_x5), BIC(modelo_gamma_log5), BIC(modelo_gamma_x6), BIC(modelo_gamma_log6))
Lista_modelos <- c(modelo_gamma_x, modelo_gamma_log, modelo_gamma_x2, modelo_gamma_log2, modelo_gamma_x3, modelo_gamma_log3, modelo_gamma_x4, modelo_gamma_log4, modelo_gamma_x5, modelo_gamma_log5, modelo_gamma_x6, modelo_gamma_log6)
min_index <- which.min(Lista_bic_modelos)
print(min_index) #con esto obtenemos el mejor modelo seleccionado a partir del criterio del BIC
menor_bic <- Lista_bic_modelos[[min_index]] #extraemos el valor del BIC y lo guardamos en una variable nueva
coef(modelo_gamma_log4)#obtenemos los coeficientes que componen el mejor modelo
BIC_GamaLigasBackForLasso<-menor_bic
sprintf("BIC: %f", BIC_GamaLigasBackForLasso)
#En este apartado lo que se busca es hacer un proceso similar al anterior generando modelo con la diferencia de que ahora se tomara el cuadrado de las variables
#MÃ©todo mejor subconjunto
ajusteCuadratico_subset <- lm(formula = brozek ~ height + I(height^2) + abdom + I(abdom^2) + wrist + I(wrist^2),data = fat)
#Metodo stepwise(Forward)
ajusteCuadratico1 <- lm(formula = brozek ~ weight + I(weight^2) + abdom + I(abdom^2) + wrist + I(wrist^2),
data = fat)
#Backward
ajusteCuadratico2 <- lm(formula = brozek ~ age + I(age^2) + abdom + I(abdom^2) + wrist + I(wrist^2),data = fat)
#Lasso
ajusteCuadratico_Lasso <- lm(formula = brozek ~ age + I(age^2) + height + I(height^2) + abdom + I(abdom^2) + wrist + I(wrist^2), data = fat)
#Metodo Stepwise(Backward)
ajusteCuadraticox4 <- glm(formula = brozek ~ weight + I(weight^2) + abdom + I(abdom^2) + wrist + I(wrist^2),family = Gamma(link = "identity"), data = fat)
ajusteCuadraticoLog4 <- glm(formula = brozek ~ weight + I(weight^2) + abdom + I(abdom^2) + wrist + I(wrist^2),family = Gamma(link = "log"), data = fat)
#Forward
ajusteCuadraticox5 <- glm(formula = brozek ~ abdom + I(abdom^2) + height:wrist + I(height^2):I(wrist^2) + chest:hip + I(chest^2):I(hip^2),family = Gamma(link = "identity"), data = fat)
ajusteCuadraticoLog5 <- glm(formula = brozek ~ abdom + I(abdom^2) + height:wrist + I(height^2):wrist + chest:hip + I(chest^2):hip,family = Gamma(link = "log"), data = fat)
#Lasso
ajusteCuadraticox6 <- glm(formula = brozek ~ abdom + I(abdom^2) + age:abdom + I(age^2):abdom + age:thigh + I(age^2):thigh + height:wrist + I(height^2):wrist,family = Gamma(link = "identity"), data = fat)
ajusteCuadraticoLog6 <- glm(formula = brozek ~ abdom + I(abdom^2) + age:abdom + I(age^2):abdom + age:thigh + I(age^2):thigh + height:wrist + I(height^2):wrist,family = Gamma(link = "log"), data = fat)
#print(BIC(ajusteCuadratico1))
#print(BIC(ajusteCuadratico2))
#print(BIC(ajusteCuadratico_Lasso))
#print(BIC(ajusteCuadraticox4))
#print(BIC(ajusteCuadraticoLog4))
#print(BIC(ajusteCuadraticox5))
#print(BIC(ajusteCuadraticoLog5))
#print(BIC(ajusteCuadraticox6))
#print(BIC(ajusteCuadraticoLog6))
#Mejor modelo
Lista_bic_modelos_al_cuadrado <- c(BIC(ajusteCuadratico_subset), BIC(ajusteCuadratico1), BIC(ajusteCuadratico2), BIC(ajusteCuadratico_Lasso), BIC(ajusteCuadraticox4), BIC(ajusteCuadraticoLog4), BIC(ajusteCuadraticox5), BIC(ajusteCuadraticoLog5), BIC(ajusteCuadraticox6), BIC(ajusteCuadraticoLog6))
Lista_modelos_variables_al_cuadrado <- c(ajusteCuadratico_subset, ajusteCuadratico1, ajusteCuadratico2, ajusteCuadratico_Lasso, ajusteCuadraticox4, ajusteCuadraticoLog4, ajusteCuadraticox5, ajusteCuadraticoLog5, ajusteCuadraticox6, ajusteCuadraticoLog6)
min_index_sqr <- which.min(Lista_bic_modelos_al_cuadrado)
#print(min_index_sqr)
menor_bic_cuadrado <- Lista_bic_modelos_al_cuadrado[[min_index_sqr]]
coef(ajusteCuadratico_subset)
BIC_menor_bic_cuadrado<-menor_bic_cuadrado
sprintf("BIC: %f", BIC_menor_bic_cuadrado)
#par(mfrow=c(1,1))
library(car)
fat$height_wrist<-fat$height*fat$wrist
fat$chest_hip<-fat$chest*fat$hip
modelo1<-lm(formula = brozek ~ abdom + height_wrist + chest_hip,
data = fat)
par(mfrow=c(1,2))
#par(mar=c(4, 5, 3, 1))
plot(modelo1, 1)   #linealidad
plot(modelo1, 3)   #homocedasticidad
plot(modelo1, 2)   #normalidad
plot(modelo1, 5)   #Outliers
library(car)
residualPlots_modelo1<-residualPlots(modelo1)
residualPlots_modelo1[,2]
residualPlots_modelo1[,2]
#Varianza constante
#Se basa en los errores estandarizados o estudentizados
#Mismas pruebas usadas en regresión lineal simple:
library(lmtest)
sbpt1<-lmtest::bptest(modelo1) #NO se rechaza H0 de homocedasticidad, NO hay problemas
sbpt1p<-sbpt1$p.value #p-value de la prueba Breusch Pagan estudentizado
#Normalidad
#Se basa en los residuales estandarizados o estudentizados
#Mismas pruebas que se usaron en regresi?n lineal simple:
library(broom)
Datosmodelo1=augment(modelo1)
swt1<-shapiro.test(Datosmodelo1$.std.resid) #Se rechaza H0 de normalidad, hay problemas
swt1p<-swt1$p.value
library(nortest)
kst1<-nortest::lillie.test(Datosmodelo1$.std.resid)#Se rechaza H0 de normalidad, hay problemas
kst1p<-kst1$p.value
library(tseries)
jbt1<-tseries::jarque.bera.test(Datosmodelo1$.std.resid)#Se rechaza H0 de normalidad, hay problemas
jbt1p<-jbt1$p.value
datos3  <- read.csv("Dat3Ex.csv") #Variables continuas sin escalar
datos3 <- datos3[,c("V1", "V2", "V4", "V6", "V9", "V12","V14","V16","V17","V26",
"V27","V29","V31","V34","V37")] #seleccionamos las preguntas
names(datos3)=c("Parlanchin", "Victimista", "Deprimido", "Reservado", "Relajado",
"Peleonero", "Tenso", "Entusiasta",  "Indulgente", "Asertivo",
"Frio", "Malhumorado", "Timido", "Calmado", "Grosero")#renombramos
#colSums(is.na(datos3))#Verificar si las variables tienen Na's
#library(GGally)
#X11()
#ggpairs(datos3)#Ver las relaciones y si es necesario escalar
library(factoextra)
R.CP_org=prcomp(datos3,  scale = FALSE) #obtenemos las componentes principales sin sacale
R.CP_est=prcomp(datos3, scale = TRUE) #obtenemos las componentes principales con sacale
R.CP_log=prcomp(log10(datos3), scale = FALSE)#obtenemos las componentes principales con log
#Nos apoyamos con la varianza que recuperamos para decidir
print(summary(R.CP_org), digits=3) #en 4 se acumulan 62.6% y en 5 67.9%
print(summary(R.CP_est), digits=3) #en 4 se acumula 62.21% y en 5 67.48%
print(summary(R.CP_log), digits=3) #en 4 se acumula 63.92% y en 5 65.58%
library(gridExtra)
plot_org <- fviz_eig(R.CP_org, main = "Sin escalar", choice ="variance", addlabels = TRUE, labelsize = 3,repel = TRUE)+theme(text = element_text(size = 9))
plot_est <- fviz_eig(R.CP_est, main = "Estandarizados", choice ="variance", addlabels = TRUE, labelsize = 3,repel = TRUE)+theme(text = element_text(size = 9))
plot_log <- fviz_eig(R.CP_log, main = "Logaritmica", choice ="variance", addlabels = TRUE, labelsize = 3,repel = TRUE)+theme(text = element_text(size = 9))
grid.arrange(plot_org, plot_est, plot_log, ncol = 3)
#Ahora para interpretar, hay que sacar correlaciones entre comp principles
#y las variables originales
#A mayor/menor valor en el comp pricipal hay mas "variables"
options(digits=2)
cor(cbind(R.CP_org$x[,1:4],(datos3)))
cor(cbind(R.CP_est$x[,1:4], (scale(datos3))))
cor(cbind(R.CP_log$x[,1:4], (log(datos3))))
#2. Para los datos estandarizados, las variables Deprimido, Tenso, Malhumorado, Grosero, Victimista, Peleonero y Frío son las que tienen mayor asociación positiva en el componente 1, y por otro lado Relajado, Calmado y Entusiasta son las que tienen mayor asociación negativa con el componente 1. Las variables Parlanchin y Asertivo son las de mayor asociación positiva para el componente 2, y Tímido y Reservado son las de mayor asociación negativa para el componente 2. Para el componente 3 las de mayor relación  positiva son Relajado, Frío y Calmado, mientras que para la relación negativa  con el componente 3 no hay valores mayores a 0.5 en valor absoluto. Y para el componente 4 la única variable con asociación negativa importante es indulgente, y todas las demás asociaciones son negativas menores a 0.5 en valor absoluto.
#3. Para los datos en escala logarítmica, las variables Grosero, Deprimido, Frio, Peleonero, Tenso, Malhumorado y Victimista son las que tienen mayor asociación positiva en el componente 1, y por otro lado Relajado es la que tienen mayor asociación negativa con el componente 1. Las variables Parlanchin, Asertivo y Entusiasta son las de mayor asociación positiva para el componente 2, y Tímido y Reservado son las de mayor asociación negativa para el componente 2. Para el componente 3 la de mayor relación  positiva es Relajado mientras que para la relación negativa  con el componente 3 es Tímido. Y para el componente 4 no hay valores mayores a 0.5 en valor absoluto.
plot1<-fviz_pca_var(R.CP_org,labelsize = 3,repel = TRUE,
col.var = "contrib") + theme(text = element_text(size = 7),
axis.title = element_text(size = 7.5),
axis.text = element_text(size = 7.5))
plot2<-fviz_pca_var(R.CP_est,labelsize = 3,repel = TRUE,
col.var = "contrib") + theme(text = element_text(size = 7),
axis.title = element_text(size = 7.5),
axis.text = element_text(size = 7.5))
plot3<-fviz_pca_var(R.CP_log,labelsize = 3,repel = TRUE,
col.var= "contrib")+ theme(text = element_text(size = 7),
axis.title = element_text(size = 7.5),
axis.text = element_text(size = 7.5))
grid.arrange(plot1,  plot3,  ncol=2)
library(psych)
set.seed(340)
parallel <- fa.parallel((datos3), fa="fa", n.iter=100) #Suguiere 3 factores
FE_org <- fa(datos3, cor= "cor",
covar = TRUE, nfactor = 3, rotate = "none")
FE_est <- fa(datos3, cor= "cov",
covar = TRUE, nfactor = 3, rotate = "varimax")
FE_log <- fa(log10(datos3), cor= "cor",
covar = TRUE, nfactor = 3, rotate = "none")
FE_org #Explica el 46%, no rechazamos H0 es buena idea usarlo, -192 BIC, RMSEA de 0.05
FE_est #Explica el 46%, no rechazamos H0, RMSEA de  0.05  y BIC = -192
FE_log #Excplica el 41% no rechazamos H0, RMSEA de 0.05 , TuckerL = 0.99 y BIC= -186
FE_org$communalities #¿Qué tan bien explican cada variable?
FE_est$communalities
FE_log$communalities #Este explica mejor individualmentes pero los otros en general
par( mfrow= c(1,2) )
plot4<-fa.diagram(FE_org,cut = 0.4 , main = "Sin escala")
plot5<-fa.diagram(FE_est,cut = 0.4 , main = "Estandarizados")
PC_org <-principal(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "none")
PC_Esc <-principal(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "none")
print(PC_org, cut = .5) #Acumula 61 y explica las variables en este orden: 0.42 0.27 0.19 0.13
print(PC_Esc, cut = .5) #Acumula 60 y explica en:  0.42 0.25 0.20 0.12
PC_org_varimax <-principal(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "varimax")
PC_Esc_varimax <-principal(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "varimax")
print(PC_org_varimax, cut = .5) #Acumula 61 y explica en: 0.33 0.30 0.22 0.14
print(PC_Esc_varimax, cut = .5) #Acumula 60 y explica en: 0.33 0.30 0.22 0.14
PC_org_oblimin <-principal(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "oblimin")
PC_Esc_oblimin <-principal(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "oblimin")
print(PC_org_oblimin, cut = .5) #Acumula 61 y explica en: 0.42 0.27 0.19 0.13
print(PC_Esc_oblimin, cut = .5) #Acumula 60 y explica en: 0.42 0.25 0.20 0.12
PC_org_cluster <-principal(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "cluster")
PC_Esc_cluster <-principal(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "cluster")
print(PC_org_cluster, cut = .5) #Acumula 61 y explica en: 0.33 0.30 0.20 0.17
print(PC_Esc_cluster, cut = .5) #Acumula 60 y explica en: 0.31 0.29 0.27 0.13
FA_org_varimax <-fa(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "varimax")
FA_Esc_varimax <-fa(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "varimax")
print(FA_org_varimax, cut = .5) #Acumula 46
print(FA_Esc_varimax, cut = .5) #Acumula 46
FA_org_oblimin <-fa(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "oblimin")
FA_Esc_oblimin <-fa(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "oblimin")
print(FA_org_oblimin, cut = .5) #Ambos acumulan 46
print(FA_Esc_oblimin, cut = .5)
FA_org_simplimax <-fa(datos3, cor="cov",
covar = TRUE, nfactor = 4, rotate = "simplimax")
FA_Esc_simplimax <-fa(datos3, cor="cor",
covar = TRUE, nfactor = 4, rotate = "simplimax")
print(FA_org_simplimax, cut = .5) #Acumulan 46
print(FA_Esc_simplimax, cut = .5) #Acumulan 46
CP_ord_varimax <- principal(datos3, cor="mixed",
covar = TRUE, nfactor = 4, rotate = "varimax")
CP_ord_cluster <- principal(datos3, cor="mixed",
covar = TRUE, nfactor = 4, rotate = "cluster")
FA_ord_oblimin <- fa(datos3, cor="mixed",
covar = TRUE, nfactor = 4, rotate = "oblimin")
FA_ord_simplimax <- fa(datos3, cor="mixed",
covar = TRUE, nfactor = 4, rotate = "simplimax")
print(CP_ord_varimax, cut=0.5) #Acumula 66 y explica 0.32 0.28 0.28 0.12
print(CP_ord_cluster, cut =0.5) #Acumula 66 y explica en:  0.31 0.29 0.27 0.13
print(FA_ord_oblimin, cut=.5)
print(FA_ord_simplimax, cut=.5)
fa.diagram(CP_ord_cluster, cut = .5, digits = 2)
#leemos los datos, omitimos na´s y eliminamos la primer columna
datos <- na.omit( read_csv("Dat4ExB.csv")[,-1])
par(mfrow=c(1,2))
# Compute clValid
library("clValid")
my_data <- scale(datos)
#intern <- clValid(my_data, nClust = 2:8,
#              clMethods = c("hierarchical","kmeans","pam",'clara'),
#              validation = "internal")
#plot(intern)
set.seed(340)
library("factoextra")
fig_s = fviz_nbclust(datos, FUNcluster = kmeans, method = c("silhouette"), k.max = 8, nstart = 20)
fig_s
#fig_s$data
library("NbClust")
set.seed(340)
res.nbclust <- NbClust(datos, distance = "euclidean",
min.nc = 2, max.nc = 8,
method = "complete", index ="all")
km.res <- kmeans(datos, 3, nstart = 25)
fviz_cluster(km.res, data = datos, frame.type = "convex") + theme_bw()
library("factoextra")
# Compute hierarchical clustering and cut into 3 clusters
res <- hcut(datos, k = 3, stand = TRUE)
# Visualize
fviz_dend(res, rect = TRUE, cex = 0.5,
k_colors = c("#00AFBB", "#E7B800", "#FC4E07"))
#funcion kmeans con semilla para hacerlo un proceso iterativo
set.seed(340)
# K-means, K = 3 y 20 asignaciones aleatorias de clusters iniciales
# Aqui x corresponde a los datos que ya deben estar preprocesados para mejores resultados
k.means <- kmeans(x = datos, centers = 3, nstart = 20)
# La asignacion a un cluster se puede obtener con $cluster
#table(k.means$cluster)
#Vamos a definir una funcion para facilitar el cambio de k, ademas de incluir el metodo silhoutte
data_K_means <- datos
kmeans_analysis <- function(x, max.nc = 7, k = 0, seed = 1, plot = F) {
if (k == 0) {
set.seed(seed)
if (plot)
plot(fviz_nbclust(x, FUNcluster = kmeans, method = c("silhouette"), k.max = max.nc, nstart = 20))
k <- NbClust(x, max.nc = max.nc, method = "kmeans", index = "silhouette")$Best.nc[[1]]
}
kmeans <- kmeans(x, k, nstart = 25)
return(factor(kmeans$cluster))
}
data_K_means$k <- kmeans_analysis(datos)
#Comenzaremos a ver el comportamiento por categoria de los clusters
ggpairs(data_K_means, title= "Kmeans con 2 Grupos", aes(colour = k))
# Probemos nuevamente tomando ahora 3 grupos y ver que pasa con los datos
data_K_means$k <- kmeans_analysis(datos, k = 3)
describeBy(data_K_means ~ k,mat=TRUE)
datos_k3 <- data_K_means
ggpairs(datos_k3, title="Kmeans con Tres Grupos", aes(colour = k), upper = list(continuous = wrap("cor", size = 2)))+theme_bw()
#Veamos que pasa con 4 clusters
data_K_means$k <- kmeans_analysis(datos, k = 4)
describeBy(data_K_means ~ k,mat=TRUE)
ggpairs(data_K_means, title="Kmedias con Cuatro Grupos", aes(colour = k))
#Ahora vamos a tomar los datos estandarizados como una primer transformacion de escala
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)))
ggpairs(data_K_means, title="Datos Estandarizados con Dos Grupos", aes(colour = k))
#repetimos el mismo proceso de tomar variables transformadas pero ahora con 3 grupos
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)), k = 3)
ggpairs(data_K_means, title="Datos Estandarizados con Tres Grupos", aes(colour = k))
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)), k = 4)
ggpairs(data_K_means, title="Datos Estandarizados con Cuatro Grupos", aes(colour = k))
R.CP <- prcomp(datos, scale = T)
fviz_pca_var(R.CP,labelsize = 3,repel = TRUE,
col.var = "contrib") + theme(text = element_text(size = 7),
axis.title = element_text(size = 7.5),
axis.text = element_text(size = 7.5))
Nfacs <- 3  # This is for four factors. You can change this as needed.
fit <- factanal(datos, Nfacs, rotation="promax")
library(psych)
loads <- fit$loadings
fa.diagram(loads)
# Procedemos a obtener los componentes principales de los datos sin estandarizar para llegar a una conclusion y ver cuantos cluster elegir
par(mfrow = c(2,2))
par(mar = c(4, 5, 3, 1))
data_K_means$k <- kmeans_analysis(datos)
plot1<- fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = data_K_means$k,
axes = c(1, 2),
pointsize = 1.5,
submain="Kmedias con 2 Categorias")
data_K_means$k <- kmeans_analysis(datos, k=3)
plot2<- fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = data_K_means$k,
axes = c(1, 2),
pointsize = 1.5,
submain="Kmedias con 3 Categorias")
data_K_means$k <- kmeans_analysis(datos, k=4)
plot3<- fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = data_K_means$k,
axes = c(1, 2),
pointsize = 1.5,
submain="Kmedias con 4 Categorias")
grid.arrange(plot1,plot2,plot3, ncol=2)
dataH <- datos
distances <- c("euclidian", "maximum", "canberra", "manhattan", "minkowski")
clustDistances <- c("ward.D", "ward.D2", "single", "complete")
hclust_analysis <- function(datos, distance, clustDist) {
for (s1 in distance) {
dis <- dist(datos, method = s1)
for (s2 in clustDist) {
jer <- hclust(dis, method = s2)
plot(jer, main = paste(s1, s2))
}
}
}
hclust_analysis(datos, distances, clustDistances)
hclust_analysis(as.data.frame(scale(datos)), distances, clustDistances)
hEucD <- hclust(dist(datos), method = "ward.D")
hEucD2 <- hclust(dist(datos), method = "ward.D2")
hMaxD <- hclust(dist(datos, method = "maximum"), method = "ward.D")
hMaxD2 <- hclust(dist(datos, method = "maximum"), method = "ward.D2")
hCanD <- hclust(dist(datos, method = "canberra"), method = "ward.D")
hCanD2 <- hclust(dist(datos, method = "canberra"), method = "ward.D2")
hManD <- hclust(dist(datos, method = "manhattan"), method = "ward.D")
hManD2 <- hclust(dist(datos, method = "manhattan"), method = "ward.D2")
hMinD <- hclust(dist(datos, method = "minkowski"), method = "ward.D")
hMinD2 <- hclust(dist(datos, method = "minkowski"), method = "ward.D2")
dataH$c <- factor(cutree(hManD2, k = 2)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc2 <- dataH
plot11 <-fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = dataH$c,
axes = c(1, 2),
pointsize = 1.5,
submain="Mét Jerar 2 Gpos, Manhattan-Ward D2")
dataH$c <- factor(cutree(hManD2, k = 3)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc3 <- dataH
plot12 <-fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = dataH$c,
axes = c(1, 2),
pointsize = 1.5,
submain="Mét Jerar 3 Gpos, Manhattan-Ward D2")
dataH$c <- factor(cutree(hManD2, k = 4)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc4 <- dataH
plot13 <-fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = dataH$c,
axes = c(1, 2),
pointsize = 1.5,
submain="Mét Jerar 4 Gpos, Manhattan-Ward D2")
grid.arrange(plot11,plot12,plot13, ncol=2)
ggpairs(dataHc2, title="Método Jerarquico con Dos Grupos, Manhattan-Ward D2", aes(colour = c))
ggpairs(dataHc3, title="Método Jerarquico con Tres Grupos, Manhattan-Ward D2", aes(colour = c))
summary(R.CP) #podemos ver que hasta la componente 4 se conserva un 79.7% de la variabilidad por lo que tomaremos estas 4 componentes para el análisis
data_pc <- as.data.frame(R.CP$x[,1:4])
dataPCK <- data_pc
dataPCK$k <- kmeans_analysis(data_pc, k=3)
dataPCK$k <- factor(dataPCK$k, levels = c("1", "2","3"), labels = c("3","2", "1"))
dataPCK3<- dataPCK
#Probamos nuevamente a intentar separar en cuatro clusters ahora con las componentes principales y tuvimos un mejor resultado que con los aglomeramientos jerÃ¡rquicos como se puede ver en las siguientes grÃ¡ficas
dataPCK$k <- kmeans_analysis(data_pc, k=4)
dataPCK$k <- factor(dataPCK$k, levels = c("1", "2","3","4"), labels = c("4","3","2", "1"))
dataPCK4<- dataPCK
ggpairs(dataPCK3, title="Kmedias CP, Tres Grupos", aes(colour = k))
plot22<-fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = dataPCK3$k,
axes = c(1, 2),
pointsize = 1.5,
submain="kmedias CP, Tres Grupos")
ggpairs(dataPCK4, title="Kmedias CP, Cuatro Grupos", aes(colour = k))
plot24<-fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = dataPCK4$k,
axes = c(1, 2),
pointsize = 1.5,
submain="kmedias CP, Cuatro Grupos")
grid.arrange(plot22,plot24, ncol=2)
#Mostramos los pesos de la componente a continuacion:
R.CP$rotation[,1] # Cargas de la primera CP
dataPCH <- data_pc
hclust_analysis(dataPCH, distances, clustDistances)
hEucD <- hclust(dist(data_pc), method = "ward.D")
hEucD2 <- hclust(dist(data_pc), method = "ward.D2")
hMaxD <- hclust(dist(data_pc, method = "maximum"), method = "ward.D")
hMaxD2 <- hclust(dist(data_pc, method = "maximum"), method = "ward.D2")
hCanD <- hclust(dist(data_pc, method = "canberra"), method = "ward.D")
hCanD2 <- hclust(dist(data_pc, method = "canberra"), method = "ward.D2")
hManD <- hclust(dist(data_pc, method = "manhattan"), method = "ward.D")
hManD2 <- hclust(dist(data_pc, method = "manhattan"), method = "ward.D2")
hMinD <- hclust(dist(data_pc, method = "minkowski"), method = "ward.D")
hMinD2 <- hclust(dist(data_pc, method = "minkowski"), method = "ward.D2")
dataPCH$c <- factor(cutree(hEucD2, 3))
plot3<-fviz_pca_ind(R.CP, geom.ind = "point",
col.ind = dataPCH$c,
axes = c(1, 2),
pointsize = 1.5,
submain="Jerarquico CP, Tres Grupos, Euclidea-Ward D2")
plot3
ggpairs(dataPCH, title="Jerarquico CP, Tres Grupos, Euclidea-Ward D2", aes(colour = c))
grid.arrange(plot22,plot3, ncol=2)
