---
output:
  pdf_document:
    toc: false
  bookdown::pdf_document2:
    number_sections: false
    toc: false
    highlight: tango
geometry: margin=2.0cm
header-includes:
- \usepackage[spanish]{babel}
- \usepackage[utf8]{inputenc}
- \usepackage{amsmath}
- \decimalpoint
urlcolor: blue
---

```{r setup, include=FALSE}
#Empezamos limpiando el espacio de trabajo 
rm(list = ls(all.names = TRUE))

# ConfiguraciÃ³n global de los bloques de cÃ³digo (chunk's)
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	#fig.dim = c(5.9, 4.9),
	fig.pos = "H",
	message = FALSE,
	warning = FALSE,
	error = F
)



# Librerias
library(dplyr)      
library(ggplot2)    
library(kableExtra) 
library(GGally)     
library(multcomp)   
library(car)        
library(broom)
library(DHARMa) 
library(ggResidpanel)
library(data.table)
library(factoextra)
library(NbClust)
library(psych)
library(gridExtra)
library(readr)
```

\section{ 4. Análisis de conglomerados}

El objetivo del analisis es identificar grupos de clientes  para focalizar la publicidad de Oddjob Airways, a partir de una encuesta resumida en ``Dat4ExB.csv``, y cuyas respuestas van de 1 a 100 (100 es que la persona considera que un aspecto es crucial en el servicio, mientras que 1 corresponde a que no lo es). Estos aspectos son puntualidad (e1), servicio según lo ofrecido (e2), experiencia placentera (e5), comodidad (e8), seguridad (e9), estado del avión (e10), comida adecuada (e16), hospitalidad (e17), viajar de forma sencilla (e21) y entretenimiento a bordo (e22). Como primer paso vamos a considerar que las variables son continuas, entonces dado ese supuesto obtendremos algunos grupos considerando el método k-means. 

```{r LeerDatos2, include=F}
#leemos los datos, omitimos na´s y eliminamos la primer columna
datos <- na.omit( read_csv("Dat4ExB.csv")[,-1])
```


Aún cuando el indicador de ``Average Silhouette width`` y los indicadores de ``Connectivity`` y ``Dunn``  muestran que el número óptimo de clusters es de 2, el indicador de ``Hubert statistic values`` muestra que deben de ser 3, y el indicador de ``Dindex values`` que deben de ser 5. Por lo que no hay un concenso indiscutible del número de clusters a considerar como óptimos. (Chunks clValid, fviz_nbclust_kmeans_silhouette y NbClust, lineas 66, 78 y  87). 


```{r clValid, echo=FALSE, fig.width=8, fig.height=4, message=FALSE,warning=FALSE, include=FALSE}
par(mfrow=c(1,2))
# Compute clValid
library("clValid")
my_data <- scale(datos)
#intern <- clValid(my_data, nClust = 2:8, 
#              clMethods = c("hierarchical","kmeans","pam",'clara'),
#              validation = "internal")
#plot(intern)
```


```{r fviz_nbclust_kmeans_silhouette, echo=F,message=FALSE,warning=FALSE, include=FALSE}
set.seed(340)
library("factoextra")
fig_s = fviz_nbclust(datos, FUNcluster = kmeans, method = c("silhouette"), k.max = 8, nstart = 20)
fig_s
#fig_s$data
```


```{r NbClust, echo=FALSE, include=FALSE}
library("NbClust")
set.seed(340)
res.nbclust <- NbClust(datos, distance = "euclidean",
                  min.nc = 2, max.nc = 8, 
                  method = "complete", index ="all") 
```


```{r, echo=FALSE, include=FALSE}
km.res <- kmeans(datos, 3, nstart = 25)
fviz_cluster(km.res, data = datos, frame.type = "convex") + theme_bw()
```

```{r, echo=FALSE, include=FALSE}
library("factoextra")
# Compute hierarchical clustering and cut into 3 clusters
res <- hcut(datos, k = 3, stand = TRUE)
# Visualize
fviz_dend(res, rect = TRUE, cex = 0.5,
          k_colors = c("#00AFBB", "#E7B800", "#FC4E07"))
```

Se decidió tomar al menos tres aspectos generales del servicio detectados en las variables: puntualidad y servicio según lo ofrecido; seguridad y estado del avión; y comodidad, experiencia, entretenimiento, hospitalidad y comida. Podemos focalizar la publicidad de la empresa en 3 grupos de clientes con base a estos tres aspectos.


```{r, echo=F,message=FALSE,warning=FALSE}
#funcion kmeans con semilla para hacerlo un proceso iterativo
set.seed(340)
# K-means, K = 3 y 20 asignaciones aleatorias de clusters iniciales 
# Aqui x corresponde a los datos que ya deben estar preprocesados para mejores resultados
k.means <- kmeans(x = datos, centers = 3, nstart = 20)

# La asignacion a un cluster se puede obtener con $cluster
#table(k.means$cluster)
```


```{r clusters, echo=F, include=FALSE}
#Vamos a definir una funcion para facilitar el cambio de k, ademas de incluir el metodo silhoutte
data_K_means <- datos

kmeans_analysis <- function(x, max.nc = 7, k = 0, seed = 1, plot = F) {
    if (k == 0) {
      set.seed(seed)
      if (plot)
        plot(fviz_nbclust(x, FUNcluster = kmeans, method = c("silhouette"), k.max = max.nc, nstart = 20))
      k <- NbClust(x, max.nc = max.nc, method = "kmeans", index = "silhouette")$Best.nc[[1]]
    }
  kmeans <- kmeans(x, k, nstart = 25)
  return(factor(kmeans$cluster))
}

data_K_means$k <- kmeans_analysis(datos)
```


```{r, echo=F, message=F, message=FALSE,warning=FALSE, include=FALSE}
#Comenzaremos a ver el comportamiento por categoria de los clusters
ggpairs(data_K_means, title= "Kmeans con 2 Grupos", aes(colour = k))
```


```{r, echo=FALSE, include=FALSE}
# Probemos nuevamente tomando ahora 3 grupos y ver que pasa con los datos 
data_K_means$k <- kmeans_analysis(datos, k = 3)
describeBy(data_K_means ~ k,mat=TRUE)
datos_k3 <- data_K_means
```

En la siguiente Gráfica podemos observar la asignación de clusters para tres grupos, y las correlaciones entre los aspectos: las correlaciones más altas son entre puntualidad (e1) y servicio acorde a lo ofrecido (e2) por un lado, seguridad (e9) y avión en buen estado (e10) por otro, y por otro lado experiencia placentera (e5) con comodidad (e8).  

```{r, echo=F, message=F,warning=FALSE,fig.dim = c(8, 6)}
ggpairs(datos_k3, title="Kmeans con Tres Grupos", aes(colour = k), upper = list(continuous = wrap("cor", size = 2)))+theme_bw() 
```

```{r, include=F,message=FALSE,warning=FALSE}
#Veamos que pasa con 4 clusters

data_K_means$k <- kmeans_analysis(datos, k = 4)
describeBy(data_K_means ~ k,mat=TRUE)
```


```{r, include=F, message=FALSE, warning=FALSE}
ggpairs(data_K_means, title="Kmedias con Cuatro Grupos", aes(colour = k))
```


```{r, message=F, echo=F, include=F,message=FALSE,warning=FALSE}
#Ahora vamos a tomar los datos estandarizados como una primer transformacion de escala
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos))) 
ggpairs(data_K_means, title="Datos Estandarizados con Dos Grupos", aes(colour = k))
```


```{r, message=F, echo=F, include=F, warning=FALSE}
#repetimos el mismo proceso de tomar variables transformadas pero ahora con 3 grupos
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)), k = 3)
ggpairs(data_K_means, title="Datos Estandarizados con Tres Grupos", aes(colour = k))
```


```{r, message=F, echo=F, include=F}
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)), k = 4)
ggpairs(data_K_means, title="Datos Estandarizados con Cuatro Grupos", aes(colour = k))
```


En los siguientes resultados auxiliares a este análisis, tenemos que la primera y segunda componente principal conservan una varianza de 58.1% y 9.7% respectivamente. Además si consideramos 3 factores, tenemos consistencia en lo planteado con los grupos.  En ambos casos podemos observar e1 y e2 muy correlacionados o en el mismo factor; e5, e8, e16 y e22 por otra parte; y e9 y e10 por otra parte. 

```{r CPrincipal, echo=F, message=F,warning=FALSE,fig.height=4, fig.width=4}
R.CP <- prcomp(datos, scale = T)
fviz_pca_var(R.CP,labelsize = 3,repel = TRUE,
             col.var = "contrib") + theme(text = element_text(size = 7),
        axis.title = element_text(size = 7.5),
        axis.text = element_text(size = 7.5))

Nfacs <- 3  # This is for four factors. You can change this as needed.
fit <- factanal(datos, Nfacs, rotation="promax")
library(psych)
loads <- fit$loadings
fa.diagram(loads)
```

```{r, echo=F, message=F, fig.dim = c(3.8, 2.7), fig.cap="Variables-PCA"}
# Procedemos a obtener los componentes principales de los datos sin estandarizar para llegar a una conclusion y ver cuantos cluster elegir
```

En la primera gráfica siguiente, vemos que a la derecha se encuentran los clientes potenciales con buenas expectativas en general en todas las preguntas, y a la izquierda los de regular y mala, según el primer componente principal.

```{r , echo=F, message=F, fig.height=3, fig.width=6}
par(mfrow = c(2,2)) 
par(mar = c(4, 5, 3, 1))
data_K_means$k <- kmeans_analysis(datos)

plot1<- fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = data_K_means$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Kmedias con 2 Categorias")

data_K_means$k <- kmeans_analysis(datos, k=3)

plot2<- fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = data_K_means$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Kmedias con 3 Categorias")

data_K_means$k <- kmeans_analysis(datos, k=4)

plot3<- fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = data_K_means$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Kmedias con 4 Categorias")
grid.arrange(plot1,plot2,plot3, ncol=2)
```

De acuerdo al método silhouette, se nos sugiere tomar dos grupos, pero en nuestro caso para mejorar la interpretación decidimos que es recomendable usar más de 2. Adicionalmente podemos ver que no cambia mucho la interpretación si nos quedamos  con 3 grupos o con 4, pues cuando agrupamos en 4 grupos, el grupo 4 combina parte de los grupos 1 y 3. 

Observando los componentes principales, podemos decir que es mejor focalizar la publicidad en 3 grupos de clientes: los que esperan puntualidad y un servicio acorde a lo contratado; los que esperan seguridad y buen mantenimiento y estado del avión; y los que esperan comodidad, experiencia, hospitalidad y entretenimiento. Tal como se decidió agrupar desde un inicio.

## Metodo Jerarquico Aglomerativo

Para esto vamos a tomar que las variables son continuas como se hizo anteriormente y tomando tanto las escalas dadas como haciendo transformaciones. Ademas agregaremos las disimilaridades entre clientes y clusters.

```{r,echo=F,message=F, warning=FALSE,include=FALSE}
dataH <- datos
distances <- c("euclidian", "maximum", "canberra", "manhattan", "minkowski")
clustDistances <- c("ward.D", "ward.D2", "single", "complete")

hclust_analysis <- function(datos, distance, clustDist) {
  for (s1 in distance) {
    dis <- dist(datos, method = s1)
    for (s2 in clustDist) {
      jer <- hclust(dis, method = s2)
      plot(jer, main = paste(s1, s2))
    }
  }
}

hclust_analysis(datos, distances, clustDistances)
hclust_analysis(as.data.frame(scale(datos)), distances, clustDistances)
```


```{r Modelos, echo=F, message=F}
hEucD <- hclust(dist(datos), method = "ward.D")
hEucD2 <- hclust(dist(datos), method = "ward.D2")
hMaxD <- hclust(dist(datos, method = "maximum"), method = "ward.D")
hMaxD2 <- hclust(dist(datos, method = "maximum"), method = "ward.D2")
hCanD <- hclust(dist(datos, method = "canberra"), method = "ward.D")
hCanD2 <- hclust(dist(datos, method = "canberra"), method = "ward.D2")
hManD <- hclust(dist(datos, method = "manhattan"), method = "ward.D")
hManD2 <- hclust(dist(datos, method = "manhattan"), method = "ward.D2")
hMinD <- hclust(dist(datos, method = "minkowski"), method = "ward.D")
hMinD2 <- hclust(dist(datos, method = "minkowski"), method = "ward.D2")
```


```{r, echo=F, message=F,warning=FALSE, fig.height=3, fig.width=6}
dataH$c <- factor(cutree(hManD2, k = 2)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc2 <- dataH
plot11 <-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Mét Jerar 2 Gpos, Manhattan-Ward D2")

dataH$c <- factor(cutree(hManD2, k = 3)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc3 <- dataH
plot12 <-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Mét Jerar 3 Gpos, Manhattan-Ward D2")

dataH$c <- factor(cutree(hManD2, k = 4)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc4 <- dataH
plot13 <-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Mét Jerar 4 Gpos, Manhattan-Ward D2")

grid.arrange(plot11,plot12,plot13, ncol=2)
```


En esta figura podemos observar las mismas comparaciones que realizamos en el ejercicio 1 donde se puede ver que el resultado obtenido en este caso aplicando el método aglomerativo resulto ser muy similar al obtenido con K-means. En esta ocasión los 3 grupos de clientes son: los que esperan puntualidad y servicio acorde a lo ofrecido, los que esperan seguridad y buen estado y mantenimiento del avión, y los que esperan experiencia placentera, comodidad, hospitalidad y entretenimiento. 


```{r Grafica45, echo=F, message=F, include=FALSE, fig.dim = c(6, 5),}
ggpairs(dataHc2, title="Método Jerarquico con Dos Grupos, Manhattan-Ward D2", aes(colour = c))
```


```{r, echo=F, message=F,warning=FALSE,echo=FALSE,include=FALSE}
ggpairs(dataHc3, title="Método Jerarquico con Tres Grupos, Manhattan-Ward D2", aes(colour = c))
```


## Modificaciones y uso de Componentes principales

```{r,echo=FALSE,message=FALSE,warning=FALSE}
summary(R.CP) #podemos ver que hasta la componente 4 se conserva un 79.7% de la variabilidad por lo que tomaremos estas 4 componentes para el análisis

data_pc <- as.data.frame(R.CP$x[,1:4])
```

Primero vamos a hacer el proceso de K-means con las 4 componentes principales que se escogieron.

```{r, message=F, echo=F, include=FALSE,warning=FALSE}
dataPCK <- data_pc
dataPCK$k <- kmeans_analysis(data_pc, k=3)
dataPCK$k <- factor(dataPCK$k, levels = c("1", "2","3"), labels = c("3","2", "1"))
dataPCK3<- dataPCK
#Probamos nuevamente a intentar separar en cuatro clusters ahora con las componentes principales y tuvimos un mejor resultado que con los aglomeramientos jerÃ¡rquicos como se puede ver en las siguientes grÃ¡ficas
dataPCK$k <- kmeans_analysis(data_pc, k=4)
dataPCK$k <- factor(dataPCK$k, levels = c("1", "2","3","4"), labels = c("4","3","2", "1"))
dataPCK4<- dataPCK
```


```{r, message=F, echo=F, include=FALSE,warning=FALSE}
ggpairs(dataPCK3, title="Kmedias CP, Tres Grupos", aes(colour = k))

plot22<-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataPCK3$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="kmedias CP, Tres Grupos")

ggpairs(dataPCK4, title="Kmedias CP, Cuatro Grupos", aes(colour = k))

plot24<-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataPCK4$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="kmedias CP, Cuatro Grupos")

grid.arrange(plot22,plot24, ncol=2)
```


```{r, echo=F, include=FALSE,message=FALSE,warning=FALSE}
#Mostramos los pesos de la componente a continuacion:
R.CP$rotation[,1] # Cargas de la primera CP
```


Análogamente a los ejercicios anteriores vamos a probar usando clusters jerarquicos y conservando las disimilaridades que se usaron en el caso anterior

```{r jerÃ¡rquicos CP, include=F}
dataPCH <- data_pc

hclust_analysis(dataPCH, distances, clustDistances)
```


Obtuvimos que los mejores modelos a usar para 3 clusters fueron Euclidean, Minkowski y Ward D2.

```{r Modelos CP, message=F, echo=F, include=FALSE, fig.height=3, fig.width=6}
hEucD <- hclust(dist(data_pc), method = "ward.D")
hEucD2 <- hclust(dist(data_pc), method = "ward.D2")
hMaxD <- hclust(dist(data_pc, method = "maximum"), method = "ward.D")
hMaxD2 <- hclust(dist(data_pc, method = "maximum"), method = "ward.D2")
hCanD <- hclust(dist(data_pc, method = "canberra"), method = "ward.D")
hCanD2 <- hclust(dist(data_pc, method = "canberra"), method = "ward.D2")
hManD <- hclust(dist(data_pc, method = "manhattan"), method = "ward.D")
hManD2 <- hclust(dist(data_pc, method = "manhattan"), method = "ward.D2")
hMinD <- hclust(dist(data_pc, method = "minkowski"), method = "ward.D")
hMinD2 <- hclust(dist(data_pc, method = "minkowski"), method = "ward.D2")

dataPCH$c <- factor(cutree(hEucD2, 3))
plot3<-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataPCH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Jerarquico CP, Tres Grupos, Euclidea-Ward D2")
plot3
ggpairs(dataPCH, title="Jerarquico CP, Tres Grupos, Euclidea-Ward D2", aes(colour = c))
```


```{r, message=F, echo=F,warning=FALSE,fig.height=2.5, fig.width=6}
grid.arrange(plot22,plot3, ncol=2)
```

## Conclusiones

Como pudimos ver a lo largo de todo este análisis y aplicando distintos métodos de evaluación como fue usar K-means, algoritmos de jerarquía y componentes principales decidimos conservar el de Componentes principales ya que ademas de permitirnos conservar las variables que conservan mayor información dadas las originales y así poder reducir el estudio a estas los resultados obtenidos fueron mas cercanos a lo que deseamos, por ejemplo, la clusterizacion que obtuvimos con la primer componente fue mejor, lo mismo pasó para la segunda componente. Hablando en términos mas generales tenemos que el primer grupo tiene mayor promedio en todas las respuestas, seguido por el segundo grupo y por ultimo se queda el tercer grupo.

Finalmente, creemos que el modelo a utilizar para focalizar la publicidad al publico siempre dependerá en gran medida de el numero de la cantidad de publico que quiera alcanzar la empresa y conforme a esto lanzar los distintos tipos de publicidad, ya que nosotros decidimos tomar 3 clasificaciones sobre 2 0 4, esto con el fin de mantener un equilibrio entre las preferencias de todos los clientes que buscan seguridad, puntualidad y un buen trato por parte de los trabajadores, cosas que sin duda son fundamentales para que la empresa logre atraer nuevos clientes potenciales que le den una gran importancia a estos criterios.









