---
title: "Analisis de Conglomerados"
author: "Saúl & Leobardo"
date: "2024-05-14"
output: pdf_document
---

```{r setup, include=FALSE}
#Empezamos limpiando el espacio de trabajo 
rm(list = ls(all.names = TRUE))

# ConfiguraciÃ³n global de los bloques de cÃ³digo (chunk's)
knitr::opts_chunk$set(
	echo = FALSE,
	fig.align = "center",
	fig.dim = c(5.9, 4.9),
	fig.pos = "H",
	message = FALSE,
	warning = FALSE,
	error = F
)



# Librerias
library(dplyr)      
library(ggplot2)    
library(kableExtra) 
library(GGally)     
library(multcomp)   
library(car)        
library(broom)
library(DHARMa) 
library(ggResidpanel)
library(data.table)
library(factoextra)
library(NbClust)
library(psych)
library(gridExtra)
library(readr)
```


El objetivo en este analisis es identificar grupos de clientes que en un futuro se puedan usar para focalizar la publicidad de la empresa.

```{r LeerDatos2, include=F}
#leemos los datos, omitimos na´s y eliminamos la primer columna que no nos servira de nada
datos <- na.omit( read_csv("Dat4ExB.csv")[,-1])
```


Como primer paso vamos a considerar que las variables son del tipo continuas, entonces dado ese supuesto obtendremos algunos grupos considerando el método k-means.

```{r, echo=F,message=FALSE,warning=FALSE}
#funcion kmeans con semilla para hacerlo un proceso iterativo
set.seed(2)
# K-means, K = 3 y 20 asignaciones aleatorias de clusters iniciales 
# Aqui x corresponde a los datos que ya deben estar preprocesados para mejores resultados
k.means <- kmeans(x = datos, centers = 3, nstart = 20)

# La asignaciÃ³n a un cluster se puede obtener con $cluster
table(k.means$cluster)
```

De las preguntas que se nos proporcionaron podemos inferir que hacen referencia a la experiencia que la Aerolínea le brinda al usuario con cosas como la limpieza del avión, la comodidad, los alimentos y la atención que le brinda el personal, ademas de hacerle su experiencia algo fácil y entretenido.
Dados estos puntos podemos focalizar la publicidad de la empresa en 3 grupos de clientes que van a estar divididos en:

1.Los clientes que esperan que el servicio sea puntual y se lleve conforme a lo planeado y una buena seguridad.

2.Los que quieren recibir un servicio seguro y con una gran experiencia de servicio por parte del personal.

3.Los que esperan un servicio puntual, una gran experiencia por parte del personal y que sea seguro para ellos.

Dadas esas clasificaciones que tomamos vamos a diseñar modelos que nos sirvan para clasificar a los clientes en estas 3 categorías en función de las expectativas que tienen del servicio que ofrece la Aerolínea.


```{r, echo=F, include=FALSE,message=FALSE,warning=FALSE}
set.seed(2)
fig_s = fviz_nbclust(datos, FUNcluster = kmeans, method = c("silhouette"), k.max = 8, nstart = 20)
fig_s
fig_s$data
```



```{r clusters, echo=F, include=FALSE}
#Vamos a definir una funcion para facilitar el cambio de k, ademas de incluir el metodo silhoutte

data_K_means <- datos

kmeans_analysis <- function(x, max.nc = 7, k = 0, seed = 1, plot = F) {
    if (k == 0) {
      set.seed(seed)
      if (plot)
        plot(fviz_nbclust(x, FUNcluster = kmeans, method = c("silhouette"), k.max = max.nc, nstart = 20))
      k <- NbClust(x, max.nc = max.nc, method = "kmeans", index = "silhouette")$Best.nc[[1]]
    }
  
  kmeans <- kmeans(x, k, nstart = 25)
  return(factor(kmeans$cluster))
}

data_K_means$k <- kmeans_analysis(datos)
```



```{r, echo=F, message=F, include = F, message=FALSE,warning=FALSE}
#Comenzaremos a ver el comportamiento por categoria de los clusters

ggpairs(data_K_means, title= "Kmeans con 2 Grupos", aes(colour = k))

```

De esta primer grafica podemos ver que el grupo  se concentra mas que el segundo grupo, es decir, muestran mas expectativas altas por el servicio que la aerolinea les ofrece mientras que el segundo grupo parece no tener expectativas tan grandes.



```{r, include=F}
# Probemos nuevamente tomando ahora 3 grupos y ver que pasa con los datos 
data_K_means$k <- kmeans_analysis(datos, k = 3)
describeBy(data_K_means ~ k,mat=TRUE)
datos_k3 <- data_K_means
```


```{r, echo=F, message=F,warning=FALSE,fig.dim = c(3.8, 2.8)}
par(nrow(c(2,1)))
ggpairs(datos_k3, title="Kmedias con Tres Grupos", aes(colour = k)) 

R.CP <- prcomp(datos, scale = T)
fviz_pca_var(R.CP,
             col.var = "contrib") # axes=c(2,3)
```


En esta nueva gráfica con 3 grupos vemos que el mas alto ahora fue el tercero con expectativas promedio mas altas, seguido de el grupo 1 y el grupo 2 respectivamente.

```{r, include=F,message=FALSE,warning=FALSE}
#Veamos que pasa con 4 clusters

data_K_means$k <- kmeans_analysis(datos, k = 4)
describeBy(data_K_means ~ k,mat=TRUE)
```


```{r, include=F, message=FALSE, warning=FALSE}
ggpairs(data_K_means, title="Kmedias con Cuatro Grupos", aes(colour = k))
```


```{r, message=F, echo=F, include=F,message=FALSE,warning=FALSE}
#Ahora vamos a tomar los datos estandarizados como una primer transformacion de escala
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos))) 
ggpairs(data_K_means, title="Datos Estandarizados con Dos Grupos", aes(colour = k))
```


```{r, message=F, echo=F, include=F, warning=FALSE}
#repetimos el mismo proceso de tomar variables transformadas pero ahora con 3 grupos
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)), k = 3)
ggpairs(data_K_means, title="Datos Estandarizados con Tres Grupos", aes(colour = k))
```


```{r, message=F, echo=F, include=F}
data_K_means$k <- kmeans_analysis(as.data.frame(scale(datos)), k = 4)
ggpairs(data_K_means, title="Datos Estandarizados con Cuatro Grupos", aes(colour = k))
```


En la siguiente gráfica tenemos que la primera y segunda componente principal conservan una varianza de 58.1% y 9.7% respectivamente.

```{r, echo=F, message=F, fig.dim = c(3.8, 2.7), fig.cap="Variables-PCA"}
# Procedemos a obtener los componentes principales de los datos sin estandarizar para llegar a una conclusiÃ³n y ver cuÃ¡ntos cluster elegir
```


```{r , echo=F, message=F, fig.dim = c(4.8, 3.8)}

par(mfrow = c(2,2)) 
par(mar = c(4, 5, 3, 1))
data_K_means$k <- kmeans_analysis(datos)

plot1<- fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = data_K_means$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Kmedias con 2 Categorias")

data_K_means$k <- kmeans_analysis(datos, k=3)

plot2<- fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = data_K_means$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Kmedias con 3 Categorias")

data_K_means$k <- kmeans_analysis(datos, k=4)

plot3<- fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = data_K_means$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Kmedias con 4 Categorias")
grid.arrange(plot1,plot2,plot3, ncol=2)


# Para el caso de variables como lo son la "e9" y "e10", las cuales hacen referencia a la seguridad y mantenimiento de el avion se encuentran mas  correlacionadas a la componente 1, es decir, a mayor valor en la componente principal 1, entonces mejores expectativas de los clientes potenciales a la seguridad y mantenimiento.

#y para las variables "e22" y "e16" que hacen referencia a la calidad del servicio, notemos que se encuentran en el cuadrante 1. A mayor valor en las expectativas de estas variables, entonces mayor valor en la componente 2.

# Por todo lo que mencionamos concluimos  que el grupo 1 trata de puntualidad y seguridad, el grupo 2 de seguridad y el grupo 3 de calidad de servicio, puntualidad y seguridad. Tal como habiamos decidido calsificar los grupos desde el principio.
```

En la  primer gráfica, vemos que a la derecha se encuentran los clientes potenciales con buenas expectativas en general en todas las preguntas del cuestionario, mientras que a la izquierda los de regular y mala, según la primera componente principal.

De acuerdo al método silhouette utilizado ,se nos sugiere tomar dos grupos, pero en nuestro caso para mejorar la interpretación decidimos que es recomendable usar más de 2. Adicionalmente podemos ver que no cambia mucho la interpretación si nos quedamos  con 3 grupos o con 4, pues cuando agrupamos en 4 la concentración de personas en el grupo 3 es muy pequeña.

Observando los componentes principales, podemos decir que es mejor focalizar la publicidad en 3 grupos de clientes: los que esperan puntualidad y seguridad, los que esperan seguridad y los que esperan calidad de servicio, puntualidad y seguridad. Tal como se decidió agrupar desde un inicio.

## Metodo Jerarquico Aglomerativo

Para esto vamos a tomar que las variables son continuas como se hizo anteriormente y tomando tanto las escalas dadas como haciendo transformaciones.Ademas agregaremos las disimilaridades entre clientes y clusters.



```{r,echo=F,message=F, warning=FALSE,include=FALSE}
dataH <- datos
distances <- c("euclidian", "maximum", "canberra", "manhattan", "minkowski")
clustDistances <- c("ward.D", "ward.D2", "single", "complete")

hclust_analysis <- function(datos, distance, clustDist) {
  for (s1 in distance) {
    dis <- dist(datos, method = s1)
    for (s2 in clustDist) {
      jer <- hclust(dis, method = s2)
      plot(jer, main = paste(s1, s2))
    }
  }
}

hclust_analysis(datos, distances, clustDistances)
hclust_analysis(as.data.frame(scale(datos)), distances, clustDistances)
```


```{r Modelos, echo=F, message=F}
hEucD <- hclust(dist(datos), method = "ward.D")
hEucD2 <- hclust(dist(datos), method = "ward.D2")
hMaxD <- hclust(dist(datos, method = "maximum"), method = "ward.D")
hMaxD2 <- hclust(dist(datos, method = "maximum"), method = "ward.D2")
hCanD <- hclust(dist(datos, method = "canberra"), method = "ward.D")
hCanD2 <- hclust(dist(datos, method = "canberra"), method = "ward.D2")
hManD <- hclust(dist(datos, method = "manhattan"), method = "ward.D")
hManD2 <- hclust(dist(datos, method = "manhattan"), method = "ward.D2")
hMinD <- hclust(dist(datos, method = "minkowski"), method = "ward.D")
hMinD2 <- hclust(dist(datos, method = "minkowski"), method = "ward.D2")
```



```{r, echo=F, message=F,warning=FALSE}
dataH$c <- factor(cutree(hManD2, k = 2)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc2 <- dataH
plot11 <-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Método Jerarquico con Dos Grupos, Manhattan-Ward D2")

dataH$c <- factor(cutree(hManD2, k = 3)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc3 <- dataH
plot12 <-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Método Jerarquico con Tres Grupos, Manhattan-Ward D2")

dataH$c <- factor(cutree(hManD2, k = 4)) # cambiando el valor de k y el cluster graficamos las distintas opciones
dataHc4 <- dataH
plot13 <-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Método Jerarquico con Cuatro Grupos, Manhattan-Ward D2")

grid.arrange(plot11,plot12,plot13, ncol=2)
```


En esta figura podemos observar las mismas comparaciones que realizamos en el ejercicio 1 donde se puede ver que el resultado obtenido en este caso aplicando el método aglomerativo resulto ser muy similar al obtenido con K-means. En esta ocasión los 3 grupos de clientes son: los que esperan puntualidad y seguridad, los que esperan calidad de servicio, puntualidad y seguridad, y los que esperan seguridad solamente.Algo que notamos que cambio fue en los promedio de expectativas de los clientes donde los clientes que se enfocan mas en la seguridad ya se encuentran mas cerca de los que tiene mas expectativa tanto en la puntualidad como en la seguridad.


```{r Grafica45, echo=F, message=F, include=FALSE, fig.dim = c(6, 5),}
ggpairs(dataHc2, title="Método Jerarquico con Dos Grupos, Manhattan-Ward D2", aes(colour = c))
```


```{r, echo=F, message=F,warning=FALSE,echo=FALSE,include=FALSE}
ggpairs(dataHc3, title="Método Jerarquico con Tres Grupos, Manhattan-Ward D2", aes(colour = c))
```


## Modificaciones y uso de Componentes principales

```{r,echo=FALSE,message=FALSE,warning=FALSE}
summary(R.CP) #podemos ver que hasta la componente 4 se conserva un 79.7% de la variabilidad por lo que tomaremos estas 4 componentes para el análisis

data_pc <- as.data.frame(R.CP$x[,1:4])
```


Primero vamos a hacer el proceso de K-means con las 4 componentes principales que se escogieron.

```{r, message=F, echo=F, include=FALSE,warning=FALSE}
dataPCK <- data_pc
dataPCK$k <- kmeans_analysis(data_pc, k=3)
dataPCK$k <- factor(dataPCK$k, levels = c("1", "2","3"), labels = c("3","2", "1"))
dataPCK3<- dataPCK
#Probamos nuevamente a intentar separar en cuatro clusters ahora con las componentes principales y tuvimos un mejor resultado que con los aglomeramientos jerÃ¡rquicos como se puede ver en las siguientes grÃ¡ficas
dataPCK$k <- kmeans_analysis(data_pc, k=4)
dataPCK$k <- factor(dataPCK$k, levels = c("1", "2","3","4"), labels = c("4","3","2", "1"))
dataPCK4<- dataPCK
```


```{r, message=F, echo=F, include=FALSE,warning=FALSE}
ggpairs(dataPCK3, title="Kmedias CP, Tres Grupos", aes(colour = k))

plot22<-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataPCK3$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="kmedias CP, Tres Grupos")

ggpairs(dataPCK4, title="Kmedias CP, Cuatro Grupos", aes(colour = k))

plot24<-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataPCK4$k, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="kmedias CP, Cuatro Grupos")

grid.arrange(plot22,plot24, ncol=2)
```


```{r, echo=F, include=FALSE,message=FALSE,warning=FALSE}
#Mostramos los pesos de la componente a continuacion:
R.CP$rotation[,1] # Cargas de la primera CP
```


Análogamente a los ejercicios anteriores vamos a probar usando clusters jerarquicos y conservando las disimilaridades que se usaron en el caso anterior

```{r jerÃ¡rquicos CP, include=F}
dataPCH <- data_pc

hclust_analysis(dataPCH, distances, clustDistances)
```



Obtuvimos que los mejores modelos a usar para 3 clusters fueron Euclidean, Minkowski y Ward D2.

```{r Modelos CP, message=F, echo=F, include=FALSE}
hEucD <- hclust(dist(data_pc), method = "ward.D")
hEucD2 <- hclust(dist(data_pc), method = "ward.D2")
hMaxD <- hclust(dist(data_pc, method = "maximum"), method = "ward.D")
hMaxD2 <- hclust(dist(data_pc, method = "maximum"), method = "ward.D2")
hCanD <- hclust(dist(data_pc, method = "canberra"), method = "ward.D")
hCanD2 <- hclust(dist(data_pc, method = "canberra"), method = "ward.D2")
hManD <- hclust(dist(data_pc, method = "manhattan"), method = "ward.D")
hManD2 <- hclust(dist(data_pc, method = "manhattan"), method = "ward.D2")
hMinD <- hclust(dist(data_pc, method = "minkowski"), method = "ward.D")
hMinD2 <- hclust(dist(data_pc, method = "minkowski"), method = "ward.D2")

dataPCH$c <- factor(cutree(hEucD2, 3))
plot3<-fviz_pca_ind(R.CP, geom.ind = "point", 
             col.ind = dataPCH$c, 
             axes = c(1, 2), 
             pointsize = 1.5,
             submain="Jerarquico CP, Tres Grupos, Euclidea-Ward D2")
plot3
ggpairs(dataPCH, title="Jerarquico CP, Tres Grupos, Euclidea-Ward D2", aes(colour = c))
```


```{r, message=F, echo=F,warning=FALSE,fig.dim = c(4.8, 3.8)}
grid.arrange(plot22,plot3, ncol=2)
```

## Conclusiones

Como pudimos ver a lo largo de todo este análisis y aplicando distintos métodos de evaluación como fue usar K-means, algoritmos de jerarquía y componentes principales decidimos conservar el de Componentes principales ya que ademas de permitirnos conservar las variables que conservan mayor información dadas las originales y así poder reducir el estudio a estas los resultados obtenidos fueron mas cercanos a lo que deseamos, por ejemplo, la clusterizacion que obtuvimos con la primer componente fue mejor, lo mismo paso para la segunda componente. Hablando en términos mas generales tenemos que el primer grupo tiene mayor promedio en todas las respuestas, seguido por el segundo grupo y por ultimo se queda el tercer grupo.

Finalmente, creemos que el modelo a utilizar para focalizar la publicidad al publico siempre dependerá en gran medida de el numero de la cantidad de publico que quiera alcanzar la empresa y conforme a esto lanzar los distintos tipos de publicidad, ya que nosotros concluimos en tomar 3 clasificaciones sobre 2 0 4, esto con el fin de mantener un equilibrio entre las preferencias de todos los clientes que buscan seguridad, puntualidad y un buen trato por parte de los trabajadores, cosas que sin duda son fundamentales para que la empresa logre atraer nuevos clientes potenciales que le den una gran importancia a estos criterios.









